[
  
  {
    "title": "An ETL Pipeline - QuantiQuail",
    "url": "/posts/An-ETL-Pipeline-QuantiQuail/",
    "categories": "Machine Learning, Python, Quantitative Finance, ETL Pipeline",
    "tags": "random-forest, xgboost, etl, trading, data-science",
    "date": "2025-08-07 00:00:00 -0700",
    





    
    "snippet": "An ETL Pipeline: QuantiQuailThe name is certainly memorable, isn‚Äôt it? üê¶I recently finished my initial version of a machine learning model to predict if a stock/ticker will move upwards or downward...",
    "content": "An ETL Pipeline: QuantiQuailThe name is certainly memorable, isn‚Äôt it? üê¶I recently finished my initial version of a machine learning model to predict if a stock/ticker will move upwards or downwards within a time window (currently next day). As of right now, it has a consistent accuracy on the testing set of ~45%. That level of low accuracy should be less accurate than random chance (since it is a binary classification problem - up or down - which has a random chance of 50% accuracy). However, at this point I can engineer different features and do transformations to reduce noise.The goal of this project is to establish a full ETL pipeline with an accuracy that performs better than random chance - even if only a little! It‚Äôs a difficult problem but that allows a lot of opportunities to explore different data science topics.The underlying modelThe model I chose for this is a RandomForestClassifier. The problem becomes simpler if I aim to predict up (1) or down (0) as a class, rather than quantify the movement numerically. Additionally, the classifier allows for good feature evaluation which should help with early development - see below.    # Quick output of feature importances to avoid adding noise    import pandas as pd        feature_importance_df = pd.DataFrame({            'feature': X_test.columns,            'importance': importances        }).sort_values(by='importance', ascending=False)        print(\"Feature importances:\\n\", feature_importance_df)Later, other models may perform better - such as XGBoost - but this one is insightful and quick to get up and running. XGBoost will also use decision trees, but sequentially rather than simultaneously. This allows continuous improvements on errors in earlier versions of the decision tree, illustrated below:# Round 1Tree1_predicts = [0.6, 0.3, 0.8, 0.2]  # Probability of UPActual_results = [1, 0, 1, 0]           # 1=UP, 0=DOWNErrors = [0.4, -0.3, 0.2, -0.2]        # What Tree1 got wrong# Round 2Tree2_learns_to_predict = [0.4, -0.3, 0.2, -0.2]  # The errors!Tree2_predicts = [0.3, -0.2, 0.1, -0.1]           # Not perfect, but helps# Combined prediction: Tree1 + Tree2Combined = [0.6, 0.3, 0.8, 0.2] + [0.3, -0.2, 0.1, -0.1]         = [0.9, 0.1, 0.9, 0.1]New_errors = [1, 0, 1, 0] - [0.9, 0.1, 0.9, 0.1] = [0.1, -0.1, 0.1, -0.1]# Round 3Tree3_learns_to_predict = [0.1, -0.1, 0.1, -0.1]Planned FeaturesInitially, I had chosen SPY as my target for predictions - however, this particular ticker is relatively stable. It‚Äôll be difficult to pick up patterns, so I‚Äôll need tickers with more volatility.The first feature I added was the label - an examination if the ticker provided a return over the specified period (one business day).    # Daily returns    # We need a label to target based off of this feature for regression:    # 1 for up prediction, 0 for down    def daily_returns(self, data):        data = data.copy()  # Avoid modifying the original DataFrame        data['Daily Returns'] = data['Close'].pct_change().round(4)        next_returns = data['Daily Returns'].shift(-1)                # Some returns are so close to zero that they are not significant,        # so we will use a threshold to avoid noise        threshold = 0.001        data['Label'] = -1 # Default label for insignificant returns        data.loc[next_returns &gt; threshold, 'Label'] = 1        data.loc[next_returns &lt; -threshold, 'Label'] = 0        data = data[data['Label'] != -1].dropna().reset_index(drop=True)        print(f\"Daily returns feature added with {data['Label'].value_counts().to_dict()}\")        return dataTo avoid noise, it drops anything that doesn‚Äôt meet the threshold to provide a clear picture of when the data a decisive class label. At 0.001 as the initial threshold, there will be room later on to tune this value to potentially increase accuracy.Revealing a pattern will require more data, transformed into features on the target ticker. I can compare the Relative Strength Index of another ticker to my target. For example, is there a pattern between Nvidia and TSMC? By calculating the RSI, the feature can capture patterns in the relative momentum between the two assets. In other words, is money likely to move from one similar asset to the other?Have a lookFind the repo here"
  },
  
  {
    "title": "Snake Detection Hypothesis - Neural Net Foundations",
    "url": "/posts/Snake-Detection-Hypothesis-Neural-Net-Foundations/",
    "categories": "Machine Learning, Python, Neural Net, Computer Vision Model(s)",
    "tags": "neural-net, sdh, computer-vision",
    "date": "2024-01-15 00:00:00 -0800",
    





    
    "snippet": "Neural Net FoundationsBackgroundA study by Nagoya University on the Snake Detection Hypothesis suggests that human vision is tuned to more easily detect a potential danger, snakes. By obscuring ima...",
    "content": "Neural Net FoundationsBackgroundA study by Nagoya University on the Snake Detection Hypothesis suggests that human vision is tuned to more easily detect a potential danger, snakes. By obscuring images completely and taking incremented steps (5% correction to baseline), they were able to gauge differences in how obscured an image could remain while being identified.As I‚Äôm studying neural nets, I thought as an interesting project I could replicate the study using a computer vision (CV) model. Assuming the hypothesis is true and human vision is evolutionarily tuned to more readily detect snakes, then the correlation should not be seen when tested by a CV model. Assuming that is the result, this may suggest that features specific to snakes do not make them more easily recognizable. Snakes do have a distinctive body type when compared to most other animals, including the animals used in the study.The study used snakes, birds, cats and fish. The dataset is not publicly available, but there are fortunately plenty of publicly available models suited for the project. I have chosen ImageNet, since it is a large and well known dataset.This initial NN is based off of fastai‚Äôs imagenette, which is designed to be small and capable for quick training and evaluation at the early stages of the development process. With only 10 classes, each distinct from each other, you can get a workable and performant model.Let‚Äôs start with our imports, load our data and targets, and a few custom functions to keep things clean:from fastai.vision.all import *path = Path('/Users/ossie/sdhnet/data/animals/imagenette2-160/')lbl_dict = dict(    n01440764='tench',    n02102040='English springer',    n02979186='cassette player',    n03000684='chain saw',    n03028079='church',    n03394916='French horn',    n03417042='garbage truck',    n03425413='gas pump',    n03445777='golf ball',    n03888257='parachute',    birds='bird')def label_func(fname):  return lbl_dict[parent_label(fname)]def get_lr(learn):    lr_min, lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))    res = round( ( (lr_min + lr_steep)/2 ), 5)    print('Learning rate finder result: ', res)    return resDatablocks &amp; BatchesAs part of the development process, it is encouraged to check that everything works in each step. Let‚Äôs check a batch from our DataBlock:dblock = DataBlock(blocks = (ImageBlock, CategoryBlock),                   get_items = get_image_files,                   get_y = label_func,                   splitter = GrandparentSplitter(),                   item_tfms = RandomResizedCrop(128, min_scale=0.35),                   batch_tfms = Normalize.from_stats(*imagenet_stats))dls = dblock.dataloaders(path, batch_size=64)dls.show_batch()Experimenting with different batch sizes, 64 is working well for the size of the dataset (700 - 900 images per category). There was some, but minimal drop in performance at smaller batch sizes of 32. Using a larger batch size can potentially increase performance by reducing training time, which is a high priority goal when producing your MVP.I‚Äôve run into a few issues where I had a training set, but no valid set created. Let‚Äôs make sure we‚Äôve got both in our DataLoaders:dls.train_ds(#10237) [(PILImage mode=RGB size=231x160, TensorCategory(1)),(PILImage mode=RGB size=240x160, TensorCategory(1)),(PILImage mode=RGB size=213x160, TensorCategory(1)),(PILImage mode=RGB size=228x160, TensorCategory(1)),(PILImage mode=RGB size=160x236, TensorCategory(1)),(PILImage mode=RGB size=213x160, TensorCategory(1)),(PILImage mode=RGB size=160x240, TensorCategory(1)),(PILImage mode=RGB size=213x160, TensorCategory(1)),(PILImage mode=RGB size=210x160, TensorCategory(1)),(PILImage mode=RGB size=160x236, TensorCategory(1))‚Ä¶]dls.valid_ds(#4117) [(PILImage mode=RGB size=213x160, TensorCategory(1)),(PILImage mode=RGB size=239x160, TensorCategory(1)),(PILImage mode=RGB size=160x200, TensorCategory(1)),(PILImage mode=RGB size=240x160, TensorCategory(1)),(PILImage mode=RGB size=240x160, TensorCategory(1)),(PILImage mode=RGB size=160x225, TensorCategory(1)),(PILImage mode=RGB size=235x160, TensorCategory(1)),(PILImage mode=RGB size=160x160, TensorCategory(1)),(PILImage mode=RGB size=160x225, TensorCategory(1)),(PILImage mode=RGB size=160x213, TensorCategory(1))‚Ä¶]Learners, Metrics and TrainingWith that in check, it‚Äôs time to create our learner. For the metrics, I‚Äôve chosen accuracy, in addition to F1 Score which is a combination of ‚Äòprecision‚Äô and ‚Äòrecall‚Äô. The F1 score should give us an understanding of how accurately positive or true predictions were correct (precision), and of all the potential positive instances, how many were identified (recall) as single, balanced, metric. By setting the average='micro', we will be able to see one score with each individual class weighed equally.We won‚Äôt be using a pre-trained model, as I‚Äôm looking to create a simple model with not too many features or attributes already trained into the network. The end goal is to compare the study with three to four CV models, with varying levels of training. Since this is the prototype and we want it quick to achieve a MVP, we can train on ResNet18. This is the smallest of the of the standard ResNet models, and should train the fastest. It should be easy to scale to a deeper network with more layers as the project progresses.It‚Äôs important to also tune the learning rate. My custom function here is leveraging fast.ai‚Äôs lr_find(), an example used here in the docs.learn = vision_learner(dls, resnet18, metrics=[accuracy, F1Score(average='micro')], pretrained=False)lr = get_lr(learn)Learning rate finder result:  0.00166Our learning rate is the midpoint between where the steepest decrease in loss occurs, and the minimum where our loss diverges and begins to increase. This should be a nice goldilocks zone that balances too low a learning rate (meaning many epochs needed to achieve a working model) and our learning rate being too large (accuracy does not reliably increase).learn.fit_one_cycle(10, lr)            epoch      train_loss      valid_loss      accuracy      f1_score      time                  0      1.423258      1.712628      0.478990      0.478990      00:39              1      1.623324      1.930002      0.407335      0.407335      00:38              2      1.597714      1.856789      0.448385      0.448385      00:38              3      1.467201      1.617843      0.532184      0.532184      00:37              4      1.352724      1.541756      0.515181      0.515181      00:38              5      1.218052      1.280943      0.580277      0.580277      00:38              6      1.048109      0.969238      0.684965      0.684965      00:38              7      0.921193      0.928279      0.698567      0.698567      00:38              8      0.832626      0.799489      0.748603      0.748603      00:37              9      0.779347      0.783034      0.746417      0.746417      00:38      interp = ClassificationInterpretation.from_learner(learn)interp.plot_confusion_matrix(figsize=(10, 10))The accuracy on predicting birds isn‚Äôt bad, but not quite matching the perfomance of the other targets.Matching the study‚Äôs dataUnfortunately, the data set used for the study is not publicly available. I wasn‚Äôt able to gather specific examples of size, source or other methods of data collection and so I chose ImageNet as a standard data source. The discrepancy in performance between target categories may be due to data set size (it‚Äôs on the lower end of our range), but it should be constructive to first bring our existing data closer to the study conditions. The Nagoya study used grayscale images, so converting the images and measuring performance again should give more insight into how well our model performs for our task.One simple change can convert our images to grayscale, using the Python Image Library (PIL) in our ImageBlock.dblock = DataBlock(blocks = (ImageBlock(cls=PILImageBW), CategoryBlock),                   get_items = get_image_files,                   get_y = label_func,                   splitter = GrandparentSplitter(),                   item_tfms = RandomResizedCrop(128, min_scale=0.35),                   batch_tfms = Normalize.from_stats(*imagenet_stats))dls = dblock.dataloaders(path, batch_size=64)dls.show_batch()learn = vision_learner(dls, resnet18, metrics=[accuracy, F1Score(average='micro')], pretrained=False)lr = get_lr(learn)learn.fit_one_cycle(10, lr)Learning rate finder result:  0.00289            epoch      train_loss      valid_loss      accuracy      f1_score      time                  0      2.667787      2.205018      0.347583      0.347583      00:37              1      2.083144      17.378586      0.137722      0.137722      00:37              2      1.813329      1.876499      0.431868      0.431868      00:37              3      1.767364      2.764703      0.254311      0.254311      00:37              4      1.703397      2.122186      0.382074      0.382074      00:37              5      1.481549      1.390573      0.549672      0.549672      00:37              6      1.329995      1.245291      0.599951      0.599951      00:38              7      1.162172      1.026676      0.678164      0.678164      00:37              8      1.014946      0.946047      0.691037      0.691037      00:37              9      0.947772      0.897824      0.710955      0.710955      00:37      bw_metrics = learn.recorder.metricsinterp = ClassificationInterpretation.from_learner(learn)interp.plot_confusion_matrix(figsize=(10, 10))Comparing the two confusion matrices, it looks like removing color resulted in a significant decrease in accuracy for the birds category, with some slight loss on others. In particular, it looks like bird more frequently gets confused with parachute. I‚Äôd guess this is because these two can be seen in the same context or setting - flying - and color features of things like the sky stand out. However, correlations with things like ‚Äòchain saw‚Äô and ‚Äòbirds‚Äô are less clear, so I will be doing some feature visualization down the line to better break down what‚Äôs happening here."
  },
  
  {
    "title": "Launch &amp; Learn - My First Model",
    "url": "/posts/My-First-Model/",
    "categories": "Machine Learning, Python",
    "tags": "random-forest, kaggle",
    "date": "2023-11-06 00:00:00 -0800",
    





    
    "snippet": "Launch And Learn - My First ModelLaunchAfter months of poking around Kaggle and looking into various machine learning models, or getting lost in documentation, I finally put together my first model...",
    "content": "Launch And Learn - My First ModelLaunchAfter months of poking around Kaggle and looking into various machine learning models, or getting lost in documentation, I finally put together my first model and submission. What changed was simple: I realized worrying too much about perfection meant I never made any progress.If you haven‚Äôt heard of it, there‚Äôs a fantastic course on machine learning I‚Äôve been working through, called fastai. An emphasis of the lessons there is a straight-forward suggestion to get something up and running so you can iterate and learn. After a few iterations I should see which levers are the most sensitive to improving (or degrading) the accuracy of my predictions.This first model ranked 3328 out of 15382 submissions, somewhere in the ballpark of the 20th percentile. Not too shabby, but I‚Äôm sure accuracy can be increased since I went for expediency rather than accuracy when prepping the training data. More on that below.Feel free to copy paste any of the below code to help yourself get started, or you can check out this repo on my GitHub.LearnWe of course start with the basic provided first cell.import numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)path = '/kaggle/input/titanic'train_data = pd.read_csv(path + '/train.csv')train_data.head()Now, to get started with preparing our data we can begin feature selection. Analyzing names seems like it could potentially impact accuracy negatively, as I don‚Äôt think this data can give us a meaningful correlation (at least on this data set). The expedient thing to do was to simply drop that column.# Create feature set X, along with our test dataX = train_datatest_data = pd.read_csv(path + '/test.csv')# For the names, just dropping them for nowX = X.drop('Name', axis=1)test_data = test_data.drop('Name', axis=1)Here‚Äôs where one interesting learning experience started: RandomForestRegressor cannot handle text input - it expects only numerical input. Since the ‚ÄòSex‚Äô feature should have a significant impact on model accuracy, it was important to include this feature in the training.There were no numerical features to extract for this feature, however since there were only two options (gender inclusivity not being a thing in 1912) we can easily encode the data. By swapping the category with a numerical value, we can then proceed with training our model.Fortunately, this can be easily done with pandas ‚Äòget_dummies()‚Äô function. We can actually encode multiple features with one function call, so I included the ‚ÄòEmbarked‚Äô feature which has three potential text values of Q, C or S.# RandomForestRegressor expects numerical data, and cannot handle non-numeric categories# For sex, embarked categories, we can encode them using pandas 'get_dummies'# Which will create additional columnsX = pd.get_dummies(X, columns=['Sex', 'Embarked'])test_data = pd.get_dummies(test_data, columns=['Sex', 'Embarked'])The result of ‚Äòget_dummies()‚Äô above results in the addition of 5 columns in place of the original two: 2 for ‚ÄòSex‚Äô, three for ‚ÄòEmbarked‚Äô. This allows the column to specify a true/false (or 1/0) value for each category. This is a technique known as One-Hot Encoding. See Chart 1 below.# Ticket &amp; Cabin columns contain values that are difficult to encode# Dropping those columns for now, but may revisit laterX = X.drop(['Ticket', 'Cabin'], axis=1)test_data = test_data.drop(['Ticket', 'Cabin'], axis=1)# Lastly, some rows are missing the age and fare fields, dropping those columns for nowX = X.drop(['Age', 'Fare'], axis=1)test_data = test_data.drop(['Age', 'Fare'], axis=1)# test_data = test_data.dropna(subset=['Age', 'Fare'])X.head()For the features ‚ÄòTicket‚Äô and ‚ÄòCabin‚Äô, we have a lot of missing values as well as data that is not suitable for one-hot encoding. While these columns are valuable as they are related to the class of the passenger, there‚Äôs a column indicating that information directly. While I imagine this results in some loss of accuracy, to get launched and iterate later, we will drop these columns.            PassengerId      Pclass      SibSp      Parch      Sex_female      Sex_male      Embarked_C      Embarked_Q      Embarked_S                  0      1      3      1      False      True      False      False      True              1      2      1      1      True      False      True      False      False              2      3      3      0      True      False      False      False      True              3      4      1      1      True      False      False      False      True              4      5      3      0      False      True      False      False      True      Chart 1: Results of One-Hot EncodingNow we can create our targets, y, and drop that column from the training set. I gave this its own cell in Kaggle to make it easier to view the head of X or y.# Create y targets from the modified X dataframey = X['Survived']# Drop the target column from X, which is already removed from test_dataX = X.drop('Survived', axis=1)X.head()Now, we can use a RandomForestClassifier and fit it to our data. Note that there is both RandomForestRegressor as well as the classifier - since we have a binary choice of survived or perished, we need to classify the passengers into a category. There are no gradients of how ‚Äòsurvived‚Äô the passenger was.# Create random forest regressorfrom sklearn.ensemble import RandomForestClassifierrf = RandomForestClassifier(n_estimators = 50, max_features = 'sqrt', max_depth = 5, random_state = 42).fit(X, y)Finally, we can leverage our model against test_data to get our predictions.# Get predictionspredictions = rf.predict(test_data)predictionsWe will need to create a DataFrame object to hold our data. For our submission, we are required to submit only the PassengerId and Survived features.submission = pd.DataFrame({    'PassengerId': test_data['PassengerId'],    'Survived': predictions})This last line will spit out a csv file we can then submit as our predictions for a ranking.submission.to_csv('submission.csv', index=False)I hope my learning process is helpful for anyone looking to get started with a basic machine learning model!Happy Hacking!"
  }
  
]

